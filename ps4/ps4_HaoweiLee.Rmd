---
title: 
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### CSP Assignment ps4
Module:    Computational Statistics & Probability - MADS <br>
Professor: Dr. Gregory Wheeler  <br>
Student:   Haowei Lee  <br>
Textbook:  Statistical Rethinking 2nd edition  <br>
* The pdf file is produced by knitting rmd into html and export the result as pdf.

Import libraries
```{r, message=FALSE}
library(rethinking)
```

### Question 1
Adapt the code in the book for plotting the observed data (Figure 11.4, top panel) to recreate the posterior predictions plot provided.

```{r}
# Import the data
data(chimpanzees)
d <- chimpanzees

d$treatment <- 1 + d$prosoc_left + 2*d$condition

dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment) )
```

Build the model
```{r, message=FALSE, results='hide'}
m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + b[treatment] ,
    a[actor] ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 )
  ) , data=dat_list , chains=4 , log_lik=TRUE )
```

```{r}
dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link( m11.4 , data=dat )
p_mu <- apply(p_post , 2 , mean )
p_ci <- apply(p_post, 2, PI)
```

Create the plot
```{r}
plot( NULL,xlim=c(1,28),ylim=c(0,1),xlab="",
ylab="proportion leftlever",xaxt="n",yaxt="n")
axis( 2,at=c(0,0.5,1),labels=c(0,0.5,1))
abline( h=0.5,lty=2)

for (j in 1:7)
  abline(v=(j - 1)*4 + 4.5,lwd=0.5)
for (j in 1:7)
  text((j - 1)*4 + 2.5,1.1,concat("actor",j),xpd=TRUE)
for ( j in (1:7)[-2] ) {
  lines((j - 1)*4 + c(1,3) , p_mu[(j - 1)*4 + c(1,3)] , lwd=1.5 )
  lines((j - 1)*4 + c(2,4) , p_mu[(j - 1)*4 + c(2,4)] , lwd=1.5 )
}
for ( i in 1:28 )
  lines( c(i,i), p_ci[,i] , lwd=1 )

points( 1:28 , p_mu , pch=16 , col="white" , cex=1.3 )
points( 1:28 , p_mu , pch=c(1,1,16,16) )
points( 1:28,t(p_mu),pch=16,col="white",cex=1.7)
points( 1:28,t(p_mu),pch=c(1,1,16,16),lwd=1)
yoff <-0.01

mtext( "posterior predictions\n")
```


### Question 2
```{r}
data(Wines2012) 
d <- Wines2012 
```
Inspect the original data
```{r}
options(digits = 2) #set to two decimal
precis(d, depth=2)
```

##### a. Construct index variables 
```{r}
dat_list <- list(
                 S = standardize(d$score),
                 jid = as.integer(d$judge),
                 wid = as.integer(d$wine) 
                )
```

##### b. Choose dnorm(0,0.5) to be the priors.
Because we standardize the wine score, so the values will tend to center around zero. Therefore we can choose 0.5 as our without further information.

##### c. Build a model considering variation among judges and wines.
```{r, message=FALSE, results='hide'}
model_Q2 <- ulam(
    alist(
        S ~ dnorm( mu , sigma ),
        mu <- j[jid] + w[wid],
        j[jid] ~ dnorm(0,0.5),
        w[wid] ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ), data=dat_list , chains=4 , cores=4 )
```

To revisit the details of our model
```{r}
show(model_Q2)
```
We have 2000 samples

Inspect the result with different index
```{r}
options(digits = 2) #set to two decimal
precis(model_Q2, depth=2)
```

To evaluate the MCMC model built by <code>ulam</code>, we can look at two indexes: <b>n_eff</b> and <b>Rhat</b>. As defined, if <b>n_eff</b> is lower than the number of samples, the sampling is inefficient. As for <b>Rhat</b>, if the value is above 1, then the MCMC is not converging. Based on this definition of evaluation, we know that our model has good values of these indicators because our <b>n_eff</b> is more than 2000 samples. As for <b>Rhat</b>, we have 1 for all parameters. Therefore, we can say our n_eff and That are pretty good.

However, sometimes these two indicators could be misleading, so it would be good to include some visualisation into our evaluation on our Markov chain. To do that, we can use <code>traceplot()</code>

##### d. Check if the chains is converging.
```{r, message=FALSE}
traceplot(model_Q2)
```

<p>
Traditionally, we observe three key aspects to evaluate our MCMC model when plotting it out.
(refer to the textbook)
<div>
  <ol>
    <li>Stationarity</li>
    <li>Good Mixing</li>
    <li>Convergence</li>
  <ol>
</div>
From the trace plot we have, we can see that the plots satisfy with the three evaluation components.
</p>

In addition, we can plot out the precis to observe the range of the parameters.
```{r}
plot(precis(model_Q2, 2))
```
<p>
<b>Questions</b>:
<div>
  <ul>
    <li>Is there variation among individual judges and individual wines?</li> 
    <li>Are there patterns you can notice just by plotting the differences?</li> 
    <li>Which judges have the lowest and highest ratings?</li>  
    <li>Which wines are rated worst and which best on average?</li> 
  </ul>
<div>

<b>Answers</b>:<br>
To answer the four questions above, we can observe either the <b>precis</b> result or the <b>precis plot</b>.
(The scores are already standardised above; therefore, we use standardised scores for comparison in the following analysis)

For the first two questions, we observe the precis plot and see noticeable variations among judges. j[i] for i=1:9 are values of 9 judges. Each j[i] represent the score and variation of a judge giving the score. Lower values mean that a judge gives a lower score on average, while higher values mean that a judge gives a higher score on average. As for the wine, we can observe the 20 kinds of wine at w[i] for i=1:20, representing the average score of a specific wine among all judges. Specifically, except for w[18], whose distribution is more extreme to the left side, other wines have values condense around the middle, where the value is 0. In conclusion, the variation of judges contributes more to the variation on the score than the part of the variation of wine do.

As for the ranking aspect in terms of judges and wine, we can also obtain insight from the result of precis and its plot. It is evident that j[8], the judge indicated as 8, gives a lower rating on average and gives the lowest mean score at around -0.65. On the other hand, the highest score, at around 0.80. was provided by j[5], the judge indicated as 5. As for the wine itself, w[18] has a lower score on average and has the lowest mean score at around -0.72, while w[4] wins the highest score at around 0.46.
</p>

### Question 3

standardise score and construct indicator variables
```{r}
list_Q3 <- list(
    score_sd = standardize(d$score),
    us_wine = d$wine.amer,
    us_judge = d$judge.amer,
    red_wine = ifelse(d$flight=="red",1L,0L)  # If red wine, assign 1L (Add "L" to make indexes integer)
)
```

As in question 2, we first standardize the wine score, so we  keep using the ordinary prior 0.5 for wine, judge and redwine. As for a, the intercept, we can try smaller sigma.
```{r, message=FALSE, results='hide'}
model_Q3 <- ulam(
    alist(
        score_sd ~ dnorm( mu , sigma ),
        mu <- a + US_W*us_wine + US_J*us_judge + RED*red_wine, 
        a ~ dnorm( 0 , 0.2 ),
        c(US_W, US_J, RED) ~ dnorm( 0 , 0.5 ),  # omit the  index variables
        sigma ~ dexp(1)
    ), data=list_Q3 , chains=4 , cores=4 )
```


Inspect the result
```{r}
options(digits = 2) #set to two decimal
precis(model_Q3, depth=2)
```

```{r}
plot(precis(model_Q3, 2))
```

<b>Questions</b>:
<div>
  <ul>
    <li>What do you conclude about the differences among the wines and judges?</li> 
    <li>Compare your results to the results from Problem 2.</li> 
  </ul>
</div>

<b>Answers</b>:<br>

<div>
Looking at the result of precis and its plot, we can conclude that:
  <ol>
    <li>There is not much difference between rea and white wine based on the value of US_W coefficient is around 0.</li>
    <li>American judges tend to be more generous on scoring the wine because we see that the vaulue of US_J is above 0 at around 0.23, and have positive correlation to the score.</li>
    <li>As for the wine itself, American wine have lower score in general becase we see that the vaulue of US_W is below 0 at around -0.18, implying the negative correlation to the score </li>
  <ol>
</div>

```{r}
show(model_Q3)
```


```{r}
traceplot(model_Q3)
```

Looking at the traceplot of model in question 3, we see that it also satisfy the three evaluation points: <b>stationarity</b>,  <b>good mixing</b>, and <b>convergence</b>. Though the <b>Rhat</b> for every parameters are also 1 as the previous model in question 2, the <b>n_eff</b> here is below 2000. Still, the amount of those effective sample is close to 2000, so it is a reasonable model.

One interesting observation is that when we use <code>show()</code> to inspect the training process behind our MCMC model, we find that model we built in question two takes a smaller amount of time training the Markov Chain. In each chain, they all take faster than 0.1 seconds. However, the four chains of model built in question three all take longer than 0.1 seconds. The time difference might result from the number of variables we consider. In question 3, we consider three variables, while in question to we only consider two. Based on the result we have for both models, we see that the result is not significantly different. However, if we consider time, we may want to construct a more time-efficient model.